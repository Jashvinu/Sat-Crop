{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8297120,"sourceType":"datasetVersion","datasetId":4928731},{"sourceId":8333451,"sourceType":"datasetVersion","datasetId":4948705}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets\nfrom torchvision import transforms , datasets\n#import torchvision.transforms as transforms\nfrom torch.optim import lr_scheduler\n!pip install torchsummary\nfrom torchsummary import summary\n\nimport matplotlib.pyplot as plt\nfrom torch.nn.utils import spectral_norm\n\nfrom torch.utils.data import Subset\n\nimport os\nfrom torchvision.io import read_image \nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport torch.nn as nn\nimport torch.functional as F\ntorch.manual_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:17:55.993285Z","iopub.execute_input":"2024-06-21T16:17:55.993602Z","iopub.status.idle":"2024-06-21T16:18:12.431141Z","shell.execute_reply.started":"2024-06-21T16:17:55.993579Z","shell.execute_reply":"2024-06-21T16:18:12.430123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models import inception_v3\ninception_model = inception_v3(pretrained=True)\n#inception_model.load_state_dict(torch.load(\"inception_v3_google-1a9a5a14.pth\"))\ninception_model.to(device)\ninception_model = inception_model.eval() # Evaluation mode\ninception_model.fc = nn.Identity()\ndef fid(num : int , dataset , inception_model , generator):\n    fake_features = []\n    real_features = []\n    for i in range(num):\n        z = torch.normal(mean = 0 , std = 1 , size = (1 ,1, 16 , 16)).to(device)\n        idx = torch.randint(0 , dataset.__len__() , (1,1)).item()\n        real_img = dataset[idx].to(device)\n        fake_img = generator(z)\n        \n        real_img = torch.nn.functional.pad(real_img , (127, 128  , 127 ,128))\n        fake_img  = torch.nn.functional.pad(fake_img ,(127 , 128 ,127 ,128))\n        \n        \n        real_img = real_img.unsqueeze(dim = 0)\n        #fake_img = fake_img.unsqueeze(dim = 0)\n        \n        fake_features.append(inception_model(fake_img))\n        real_features.append(inception_model(real_img))\n        \n    real_matrix = torch.stack(real_features)\n    fake_matrix = torch.stack(fake_features)\n    \n    real_mean = torch.mean(real_matrix , dim = 0)\n    fake_mean = torch.mean(fake_matrix , dim = 0)\n    real_matrix = real_matrix.squeeze(dim  = 1)\n    fake_matrix = fake_matrix.squeeze(dim  = 1)\n    \n    real_cov = torch.cov( torch.transpose(real_matrix,0 , 1 ))\n    fake_cov = torch.cov( torch.transpose(fake_matrix, 0 , 1))\n    \n    return (torch.square(torch.norm((real_mean - fake_mean), p ='fro')) + torch.trace(real_cov + fake_cov - 2*torch.sqrt(real_cov*fake_cov)))\n    \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:12.433270Z","iopub.execute_input":"2024-06-21T16:18:12.433925Z","iopub.status.idle":"2024-06-21T16:18:13.644467Z","shell.execute_reply.started":"2024-06-21T16:18:12.433895Z","shell.execute_reply":"2024-06-21T16:18:13.643573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n  def __init__(self , num , dim):\n    super(Discriminator, self ).__init__()\n\n    # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n    # padding_mode='zeros', device=None, dtype=None)\n\n    #Hout =⌊  ( Hin +2×padding[0]−dilation[0]×(kernel_size[0]−1)−1 / stride[0]) + 1  ⌋\n\n    #input is 64x64 and output is a sigmoid function\n\n    self.conv1 = spectral_norm((nn.Conv2d(3 , 64 ,kernel_size = 3 ,stride =  2 ,padding =  1 )))\n    self.batchnorm1 = nn.BatchNorm2d(64)\n    self.elu1 = nn.LeakyReLU(0.1, inplace = False)\n    self.dropout1 = nn.Dropout2d(0.375)\n    \n    self.conv2 = spectral_norm(nn.Conv2d(64 , 128,kernel_size=  3 ,stride =  2,padding =  1 ))\n    self.batchnorm2 = nn.BatchNorm2d(128)\n    self.elu2 =  nn.LeakyReLU(0.1, inplace = False)\n    self.dropout2 = nn.Dropout2d(0.375)\n\n    self.conv3 = spectral_norm((nn.Conv2d(128, 256 ,kernel_size =  3 ,stride =  2 ,padding =  1 )))\n    self.batchnorm3 = nn.BatchNorm2d(256)\n    self.elu3 = nn.LeakyReLU(0.1, inplace = False)\n    self.dropout3 = nn.Dropout2d(0.375)\n    \n    self.conv4 = spectral_norm(nn.Conv2d(256 , 512 ,kernel_size =  3 ,stride =  2 , padding = 1 ))\n    self.batchnorm4 = nn.BatchNorm2d(512)\n    self.elu4 =  nn.LeakyReLU(0.1, inplace = False)\n    self.dropout4 = nn.Dropout2d(0.375)\n    \n\n    self.leakyrelu= (nn.LeakyReLU(negative_slope =0.2))\n    self.simple = spectral_norm(nn.Linear(8192, 1))\n\n    self.dropout = nn.Dropout(p=0.60)\n    self.weights_init()\n\n    # same layers as that of the generator\n  def weights_init(m):\n    print(\"init\")\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n        #self.weight_init(m.weight.data, mean=0.0, std=0.08)\n        nn.init.xavier_uniform_(m.weight)\n        #nn.init.normal_(m.weight, mean=0.0, std=0.02, generator=None)\n  def forward(self , img):\n    h1 =self.elu1((self.conv1(img)))\n    #h1 = self.dropout1(h1)\n    h2 =self.elu2((self.conv2(h1)))\n    #h2 = self.dropout2(h2)\n    h3 =self.elu3((self.conv3(h2)))\n    #h3 = self.dropout3(h3)\n    h4 =self.elu4((self.conv4(h3)))\n    #h4 = self.dropout4(h4)\n    \n\n    h6 = torch.flatten(h4 , start_dim = 1)\n    #h6 = self.dropout(h6)\n    h7 = self.simple(h6)\n    return self.leakyrelu(h7) , h6\n\n    \n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:13.645792Z","iopub.execute_input":"2024-06-21T16:18:13.646131Z","iopub.status.idle":"2024-06-21T16:18:13.660570Z","shell.execute_reply.started":"2024-06-21T16:18:13.646093Z","shell.execute_reply":"2024-06-21T16:18:13.659614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n  # it takes in an input of a tensor (2-D) of size 8x8 in the paper it is a 1-D array , but for simplicity of convolution's dimensions i am taking y = x\n  #Hout =(Hin − 1 )×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n\n  #Wout =(Win − 1 )×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1\n\n\n\n  def __init__(self , num , dim ):\n    super(Generator, self).__init__()\n\n  # i will be inputting a tensor of size (batch_size , 1(channels) , num(y dimension) , num(x dimension))\n  # and i want a output of (batch_size , 3(channels) , dim(y dimension) , dim(x dimension))\n\n\n#torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n  #output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n    \n    #take a noise of 100 size , then make it into 1024*4*4 , then reshape it into (1 , 1024 , 4 , 4)\n\n    # i am assuming that the covolutions with stride 1 are the reason behind the patternized outputs ... bcz i dont have anything else to blame\n    #that on and also the notebook which i saw was working had this only\n    self.linear = nn.Linear(256 , 16384)\n    self.elul =  nn.LeakyReLU(0.1, inplace = False)\n    \n    self.convt1 = nn.ConvTranspose2d(in_channels =1024 , out_channels = 512 , kernel_size = 3 , stride = 2 , padding = 1, output_padding = 1)\n    self.batchnorm1 = nn.BatchNorm2d(512)\n    self.elu1 =  nn.LeakyReLU(0.1, inplace = False)\n\n    self.convt2 = nn.ConvTranspose2d(512 , 256,kernel_size =  3 ,stride =  2  , padding = 1 , output_padding = 1 )\n    self.batchnorm2 = nn.BatchNorm2d(256)\n    self.elu2 =  nn.LeakyReLU(0.1, inplace = False)\n    \n\n    self.convt3 = nn.ConvTranspose2d(256, 128 ,kernel_size =  3,stride =  2 , padding = 1 , output_padding = 1   )\n    self.batchnorm3 = nn.BatchNorm2d(128)\n    self.elu3 = nn.LeakyReLU(0.1, inplace = False)\n    \n\n    self.convt5 = nn.ConvTranspose2d(128 , 3 ,kernel_size = 3 ,stride =  2 ,padding =  1  , output_padding = 1)\n    self.batchnorm5 = nn.BatchNorm2d(1)\n    self.relu5 = nn.ReLU(inplace = False)\n    self.tanh = nn.Tanh()\n    \n    self.weights_init()\n  def weights_init(m):\n    print(\"initdone\")\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n        #self.weight_init(m.weight.data, mean=0.0, std=0.08)\n        nn.init.xavier_uniform_(m.weight)\n        #nn.init.normal_(m.weight, mean=0.0, std=0.02, generator=None)\n  def forward(self , z):\n    z = torch.flatten(z , start_dim = 1)\n    z1 = self.elul(self.linear(z))\n    \n    z2 = z1.reshape(-1 , 1024 , 4, 4)\n    h1 =self.elu1(self.batchnorm1(self.convt1(z2)))\n    h2 =self.elu2((self.convt2(h1)))\n    h3 =self.elu3((self.convt3(h2)))\n    \n    h5 =self.relu5((self.convt5(h3)))\n    \n    h6 = self.tanh(h5)\n\n    return h6\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:13.662780Z","iopub.execute_input":"2024-06-21T16:18:13.663072Z","iopub.status.idle":"2024-06-21T16:18:13.677357Z","shell.execute_reply.started":"2024-06-21T16:18:13.663048Z","shell.execute_reply":"2024-06-21T16:18:13.676436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# first create a dataset , and a dataloader \ntrans = transforms.Compose([\n   \n    transforms.Resize((64,64)), \n    torchvision.transforms.ConvertImageDtype(torch.float32)\n    \n    ])\ntrans_for_mnist =  transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((64,64)), \n    torchvision.transforms.ConvertImageDtype(torch.float32)\n    \n    ])\n\n\n#dataset class \nsubset_size = 440\nclass img_dataset(Dataset):\n  def __init__(self , img_dir , trans , subset_size):\n    self.img_dir = img_dir\n    self.transform = trans\n    self.files = os.listdir(self.img_dir)\n    self.data = []\n    self.subset_size = subset_size\n    for idx , file in enumerate(self.files):\n        if (idx > subset_size):\n            break\n        img = read_image(os.path.join(self.img_dir , file))\n        img = trans(img)\n        self.data.append(img)\n        \n  def __len__(self):\n    \n    return self.subset_size\n  def __getitem__(self ,idx ):\n    img = self.data[idx]\n    return img\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:13.678454Z","iopub.execute_input":"2024-06-21T16:18:13.678787Z","iopub.status.idle":"2024-06-21T16:18:13.688872Z","shell.execute_reply.started":"2024-06-21T16:18:13.678756Z","shell.execute_reply":"2024-06-21T16:18:13.687994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss_func = nn.BCELoss()\ngenerator = Generator(4 , 64)\ndiscriminator = Discriminator (4, 64)\n\n#trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=trans_for_mnist)\ndataset = img_dataset(\"/kaggle/input/gan-large-area-png\", trans , subset_size)\n#dataset = trainset\nbatch = 64\n\n\n\n\n\n\n\n#device thing\n# Assuming your model is named 'model'\n\ngenerator = generator.to(device)\n#discriminator.spectral_norm()\ndiscriminator = discriminator.to(device)\n\n#dataset = dataset.to(device)\n\n\ng_optim = torch.optim.Adam(generator.parameters() , lr = 0.00002 , betas = (0 , 0.9) )\nd_optim = torch.optim.Adam(discriminator.parameters() , lr = 0.0000375, betas = (0 , 0.9) )\n\n#d_optim = torch.optim.(discriminator.parameters() , lr = 0.00005, betas = (0.5 , 0.999))\n#g_optim = torch.optim.Adam(generator.parameters() , lr = 0.00005, betas = (0.5 , 0.999))\n\ng_scheduler = lr_scheduler.ExponentialLR(g_optim, gamma=0.998) \nd_scheduler = lr_scheduler.ExponentialLR(d_optim, gamma=0.998)\n\nsubset  = Subset(dataset , list(range(subset_size)))\ndata_loader = DataLoader(subset, batch_size = batch , shuffle = True , pin_memory = True)\n\n\n#shapes \n# z = (batch, 1 , 8 , 8)\nz = torch.normal(mean = 0 , std = 1 , size = (batch ,1, 16 , 16)).to(device)\nprint(\"shape of generator output = \" + str((generator(z).shape)))\nprint(\"shape of discriminator output = \" + str(discriminator(generator(z))[0].shape))\n\n\nprint(dataset.__len__())\n\nfor i in range(5):\n    img = dataset[i]\n    plt.imshow(img.permute(1,2,0))\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:13.689883Z","iopub.execute_input":"2024-06-21T16:18:13.690191Z","iopub.status.idle":"2024-06-21T16:18:17.339993Z","shell.execute_reply.started":"2024-06-21T16:18:13.690166Z","shell.execute_reply":"2024-06-21T16:18:17.339138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.__len__())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:17.341007Z","iopub.execute_input":"2024-06-21T16:18:17.341313Z","iopub.status.idle":"2024-06-21T16:18:17.346502Z","shell.execute_reply.started":"2024-06-21T16:18:17.341286Z","shell.execute_reply":"2024-06-21T16:18:17.345616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(generator , input_size = (1,16,16))\nsummary(discriminator , input_size = (3,64,64))\nsummary(inception_model, input_size = (3, 299 , 299))\n#remember the discriminator parameters shown here are double of the actual number bcz of the spectral norm thing\ngenerator = nn.DataParallel(generator)\ndiscriminator = nn.DataParallel(discriminator)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:17.347728Z","iopub.execute_input":"2024-06-21T16:18:17.348014Z","iopub.status.idle":"2024-06-21T16:18:17.649150Z","shell.execute_reply.started":"2024-06-21T16:18:17.347991Z","shell.execute_reply":"2024-06-21T16:18:17.648148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3000\niter = 4\n\nones = torch.ones(batch,1).to(device)\nzeroes = torch.zeros(batch,1).to(device)\nfidlist = []\ngenerator_loss = []\ndiscriminator_loss = []\nepoch_count = []\ndiscriminator_loss_part1 = []\ndiscriminator_loss_part2 = []\nfeature_loss_list = []\nclip_value = 1\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:17.650526Z","iopub.execute_input":"2024-06-21T16:18:17.650939Z","iopub.status.idle":"2024-06-21T16:18:17.657501Z","shell.execute_reply.started":"2024-06-21T16:18:17.650904Z","shell.execute_reply":"2024-06-21T16:18:17.656583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.utils as vutils\n\nprint(ones.dtype)\ngenerator.train()\ndiscriminator.train()\nfeature_loss =  torch.nn.MSELoss()\nfactor = 0\nfor epoch in tqdm(range(epochs)):\n\n    \n    \n  factor = factor*0.995\n  #print(epoch)\n  epoch_count.append(epoch)\n  generator_loss.append(0)\n  discriminator_loss.append(0)\n  discriminator_loss_part1.append(0)\n  discriminator_loss_part2.append(0)\n  feature_loss_list.append(0)\n  fidlist.append(fid(50 , dataset , inception_model , generator).item())\n  \n  for batch_idx , (data) in enumerate(data_loader):\n    if(batch_idx % iter == 0):\n        generator.train()\n        discriminator.eval()\n          \n        #print(\"gen----------------------------------------------\" , batch_idx)\n        batch_size = (data.shape[0])\n        data = data.to(device)\n        z = torch.normal(mean = 0 , std = 1 ,size = (batch_size , 1 , 16,16) ).to(device)\n        discriminator_output_generated , discriminator_features_generated = discriminator(generator(z))\n        discriminator_output_real, discriminator_features_real = discriminator(data)\n\n        f_loss = feature_loss(discriminator_features_generated , discriminator_features_real)\n        feature_loss_list[epoch] += f_loss.item()\n        g_loss = torch.mean(-1*(discriminator_output_generated))\n\n        g_loss = (1-factor)*g_loss+ factor*f_loss\n        generator_loss[epoch] += torch.mean(g_loss).item()\n        d_optim.zero_grad()  \n    \n        g_optim.zero_grad()\n\n        g_loss.backward()\n        #torch.nn.utils.clip_grad_norm_(generator.parameters(), clip_value) \n        g_optim.step()\n    else:\n        generator.eval()\n        discriminator.train()\n        #print(\"dis\" , batch_idx)\n        #print(batch_idx)\n        batch_size = (data.shape[0])\n        data = data.to(device)\n\n\n        z = torch.normal(mean = 0 , std = 1 ,size = (batch_size , 1 , 16,16)).to(device)\n        discriminator_output_real = discriminator(data)[0]\n        discriminator_output_generated = discriminator(generator(z))[0]\n        part1 = (discriminator_output_real)\n        part2 = ( discriminator_output_generated)\n        #d_loss = loss_func(discriminator_output_real, ones) + loss_func( discriminator_output_generated, zeroes)\n        d_loss = torch.mean(-1*(part1 - part2))\n        discriminator_loss[epoch] += (d_loss).item()/(iter -1)\n        discriminator_loss_part1[epoch] += -1*torch.mean(part1).item()/(iter-1)\n\n        discriminator_loss_part2[epoch] += torch.mean(part2).item()/(iter-1)\n        d_optim.zero_grad() \n        d_loss.backward()\n         \n        d_optim.step()\n        \n   #torch.nn.utils.clip_grad_norm_(generator.parameters(), clip_value)\n \n    if(epoch%2 == 0):\n        \n         \n            if batch_idx == 0:\n                with torch.no_grad():\n                    generator.eval()\n                    sample_z = torch.normal(mean=0, std=1, size=(1, 1, 16, 16)).to(device)\n                    generated_img = generator(sample_z).cpu()\n\n                    # Convert the image tensor to [0, 1] range and save it\n                    img_path = f'images/epoch_{epoch}_batch_{batch_idx}.png'\n                    vutils.save_image(generated_img, img_path, normalize=True)\n\n \n  #g_scheduler.step()\n  #d_scheduler.step()\n    \n\n    \n    #print(str(data.min()) + \"   \" + str(data.max()))\n    #print(str(generator(z).min()) + \"  \" + str(generator(z).max()))\n  \n  print(f\"epoch - : {epoch_count[epoch]},Generator loss - : {generator_loss[epoch]}, Discriminator - : {discriminator_loss[epoch]} , g_lr = {g_optim.param_groups[0]['lr']} , d_lr = {d_optim.param_groups[0]['lr']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:18:40.888482Z","iopub.execute_input":"2024-06-21T16:18:40.888834Z","iopub.status.idle":"2024-06-21T17:56:06.877383Z","shell.execute_reply.started":"2024-06-21T16:18:40.888808Z","shell.execute_reply":"2024-06-21T17:56:06.876479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt  # Import matplotlib for saving images\nfrom tqdm import tqdm\n\n# Assuming 'discriminator' is your pre-trained model\n\n# Define the device for computation\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load pre-trained model\nmodel = discriminator\nmodel.eval()\nmodel = model.to(device)\n\n# Define loss function (for example, maximizing activation of a certain layer)\nloss_activation = []\nloss_function = nn.MSELoss()\n\n# Initialize input image (random noise or blank image)\ninput_img = torch.randn(1, 3, 64, 64, device=device, requires_grad=True)  # or initialize with zeros or random noise\ninput_img = input_img.to(device)\n\n# Define optimizer\noptimizer = optim.Adam([input_img], lr=0.0001)\n\n# Optimization loop\nnum_iterations = 100  # Increase iterations for more refined result\nfor i in tqdm(range(num_iterations)):\n    optimizer.zero_grad()\n    output = model(input_img)[0]\n    \n    # Maximize the activation of the chosen layer (example)\n    loss = -torch.mean(model(input_img)[0])\n    loss_activation.append(loss.item())\n    \n    loss.backward()\n    optimizer.step()\n    \n    if (i + 1) % 10 == 0:  # Adjust as needed to save images periodically\n        # Convert optimized input image to PIL image for visualization\n        output_img = transforms.ToPILImage()(input_img.squeeze().detach().cpu())\n        \n        # Save the image using matplotlib\n        plt.imshow(output_img)\n        plt.axis('off')\n        plt.savefig(f'output_{i+1}.png', bbox_inches='tight')\n        plt.close()\n\n# Plot loss vs iterations2\nplt.figure()\nplt.plot(range(num_iterations), loss_activation, label='Loss Activation')\nplt.xlabel('Iterations')\nplt.ylabel('Loss Activation')\nplt.title('Loss Activation vs Iterations')\nplt.grid(True)\nplt.savefig('loss_vs_iterations.png', bbox_inches='tight')\nplt.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:25.243857Z","iopub.execute_input":"2024-06-21T18:01:25.244293Z","iopub.status.idle":"2024-06-21T18:01:27.765228Z","shell.execute_reply.started":"2024-06-21T18:01:25.244261Z","shell.execute_reply":"2024-06-21T18:01:27.764422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# Sample data for demonstration\nx_values = epoch_count\ny1_values = generator_loss\ny2_values = discriminator_loss\ny3_values = discriminator_loss_part1\ny4_values = discriminator_loss_part2\ny5_values = feature_loss_list\n\n# Plotting the first line\nplt.plot(x_values, y1_values, label='generator_loss')\n\n# Plotting the second line\nplt.plot(x_values, y2_values, label='discriminator_loss')\n\nplt.plot(x_values, y3_values, label='discriminator_loss_part1')                     \n\n\n# Plotting the second line\nplt.plot(x_values, y4_values, label='discriminator_loss_part2')\n\nplt.plot(x_values , y5_values , label= 'feature_loss')\n\n# Adding labels and title\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.title('Loss Graphs')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\nclipped_discriminator = [40 if x > 40 else -40 if x < -40 else x for x in discriminator_loss]\nplt.plot(epoch_count , clipped_discriminator , label = 'clipped_discriminator')\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.title('Discriminator_loss')\n\nplt.legend()\n\n\nplt.show()\n\n\nplt.plot(x_values, y1_values, label='generator_loss')\n\n# Plotting the second line\nplt.plot(x_values, y2_values, label='discriminator_loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss graph')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\n\n# Sample data for demonstration\nx_values = epoch_count\ny1_values = discriminator_loss_part1\ny2_values = discriminator_loss_part2\n# Plotting the first line\nplt.plot(x_values, y1_values, label='discriminator_loss_part1')\n                        \n\n# Plotting the second line\nplt.plot(x_values, y2_values, label='discriminator_loss_part2')\n\n# Adding labels and title\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('discriminator_losses_broken_down')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:32.694036Z","iopub.execute_input":"2024-06-21T18:01:32.694862Z","iopub.status.idle":"2024-06-21T18:01:34.143893Z","shell.execute_reply.started":"2024-06-21T18:01:32.694830Z","shell.execute_reply":"2024-06-21T18:01:34.142959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1_values = fidlist\nplt.plot(x_values, y1_values, label='fid score')\n\n# Plotting the second line\n\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.title('FID score ')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\ny1_values = [min(x , 100) for x in fidlist]\nplt.plot(x_values, y1_values, label='cropped fid score')\n\n# Plotting the second line\n\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.title('Cropped FID score ')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:42.357503Z","iopub.execute_input":"2024-06-21T18:01:42.357900Z","iopub.status.idle":"2024-06-21T18:01:42.968016Z","shell.execute_reply.started":"2024-06-21T18:01:42.357868Z","shell.execute_reply":"2024-06-21T18:01:42.967049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1_values = feature_loss_list\nplt.plot(x_values, y1_values, label='feature_losses')\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Feature Loss graph')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\n#cropped graph\n\ny1_values= [min(x , 50) for x in feature_loss_list]\nplt.plot(x_values, y1_values, label='cropped feature_losses')\n\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Cropped Feature Loss graph')\n\n# Adding a legend to differentiate lines\nplt.legend()\n\n# Display the plot\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:47.056655Z","iopub.execute_input":"2024-06-21T18:01:47.056997Z","iopub.status.idle":"2024-06-21T18:01:47.532407Z","shell.execute_reply.started":"2024-06-21T18:01:47.056971Z","shell.execute_reply":"2024-06-21T18:01:47.531650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Save a list to a file\n\nwith open('generator_loss.pkl', 'wb') as f:\n    pickle.dump(generator_loss, f)\n\n\nwith open('discriminator_loss.pkl', 'wb') as f:\n    pickle.dump(discriminator_loss, f)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:51.884634Z","iopub.execute_input":"2024-06-21T18:01:51.885472Z","iopub.status.idle":"2024-06-21T18:01:51.891410Z","shell.execute_reply.started":"2024-06-21T18:01:51.885439Z","shell.execute_reply":"2024-06-21T18:01:51.890480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nbase = generator(z)[0]\nlista = []\nfor i in range(1000):\n        z = torch.normal(mean = 0 , std = 1, size = (32 , 1 , 16 , 16)).to(device)\n        img = generator(z)[0]\n        diff = torch.sum(base - img)\n        lista.append(diff.item())\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:01:56.900817Z","iopub.execute_input":"2024-06-21T18:01:56.901204Z","iopub.status.idle":"2024-06-21T18:02:05.380435Z","shell.execute_reply.started":"2024-06-21T18:01:56.901173Z","shell.execute_reply":"2024-06-21T18:02:05.379445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(lista))\nplt.scatter(lista, range(0, len(lista) ))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:16.541204Z","iopub.execute_input":"2024-06-21T18:02:16.541580Z","iopub.status.idle":"2024-06-21T18:02:16.827488Z","shell.execute_reply.started":"2024-06-21T18:02:16.541551Z","shell.execute_reply":"2024-06-21T18:02:16.826509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n        \n        z = torch.normal(mean = 0 , std = 1 , size = (batch ,1, 16 , 16)).to(device)\n        img = generator(z)[0].to('cpu').detach()\n        img  = img.permute(1,2,0)\n        print(torch.min(img) , \"    \" , torch.max(img))\n        plt.imshow(img)\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:20.255052Z","iopub.execute_input":"2024-06-21T18:02:20.255443Z","iopub.status.idle":"2024-06-21T18:02:22.291799Z","shell.execute_reply.started":"2024-06-21T18:02:20.255411Z","shell.execute_reply":"2024-06-21T18:02:22.290908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fidlist","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:27.505975Z","iopub.execute_input":"2024-06-21T18:02:27.506855Z","iopub.status.idle":"2024-06-21T18:02:27.525015Z","shell.execute_reply.started":"2024-06-21T18:02:27.506821Z","shell.execute_reply":"2024-06-21T18:02:27.524072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(fidlist)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:38.010233Z","iopub.execute_input":"2024-06-21T18:02:38.010604Z","iopub.status.idle":"2024-06-21T18:02:38.016674Z","shell.execute_reply.started":"2024-06-21T18:02:38.010575Z","shell.execute_reply":"2024-06-21T18:02:38.015692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fidlist[2995:2999]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:35.684519Z","iopub.execute_input":"2024-06-21T18:02:35.685488Z","iopub.status.idle":"2024-06-21T18:02:35.691507Z","shell.execute_reply.started":"2024-06-21T18:02:35.685435Z","shell.execute_reply":"2024-06-21T18:02:35.690509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min(fidlist)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:02:40.025935Z","iopub.execute_input":"2024-06-21T18:02:40.026662Z","iopub.status.idle":"2024-06-21T18:02:40.032818Z","shell.execute_reply.started":"2024-06-21T18:02:40.026621Z","shell.execute_reply":"2024-06-21T18:02:40.031907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:07:04.172027Z","iopub.execute_input":"2024-06-21T18:07:04.172947Z","iopub.status.idle":"2024-06-21T18:07:05.515564Z","shell.execute_reply.started":"2024-06-21T18:07:04.172912Z","shell.execute_reply":"2024-06-21T18:07:05.514548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:07:29.472950Z","iopub.execute_input":"2024-06-21T18:07:29.473344Z","iopub.status.idle":"2024-06-21T18:07:30.450503Z","shell.execute_reply.started":"2024-06-21T18:07:29.473314Z","shell.execute_reply":"2024-06-21T18:07:30.449559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:07:44.769358Z","iopub.execute_input":"2024-06-21T18:07:44.770267Z","iopub.status.idle":"2024-06-21T18:07:44.778203Z","shell.execute_reply.started":"2024-06-21T18:07:44.770222Z","shell.execute_reply":"2024-06-21T18:07:44.777228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8297120,
          "sourceType": "datasetVersion",
          "datasetId": 4928731
        },
        {
          "sourceId": 8333451,
          "sourceType": "datasetVersion",
          "datasetId": 4948705
        }
      ],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "working GAN",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms , datasets\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.optim import lr_scheduler\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:17:55.993285Z",
          "iopub.execute_input": "2024-06-21T16:17:55.993602Z",
          "iopub.status.idle": "2024-06-21T16:18:12.431141Z",
          "shell.execute_reply.started": "2024-06-21T16:17:55.993579Z",
          "shell.execute_reply": "2024-06-21T16:18:12.430123Z"
        },
        "trusted": true,
        "id": "VL8jAbCRzzD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "902f65ENzzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import inception_v3\n",
        "inception_model = inception_v3(pretrained=True)\n",
        "#inception_model.load_state_dict(torch.load(\"inception_v3_google-1a9a5a14.pth\"))\n",
        "inception_model.to(device)\n",
        "inception_model = inception_model.eval() # Evaluation mode\n",
        "inception_model.fc = nn.Identity()\n",
        "def fid(num : int , dataset , inception_model , generator):\n",
        "    fake_features = []\n",
        "    real_features = []\n",
        "    for i in range(num):\n",
        "        z = torch.normal(mean = 0 , std = 1 , size = (1 ,1, 16 , 16)).to(device)\n",
        "        idx = torch.randint(0 , dataset.__len__() , (1,1)).item()\n",
        "        real_img = dataset[idx].to(device)\n",
        "        fake_img = generator(z)\n",
        "\n",
        "        real_img = torch.nn.functional.pad(real_img , (127, 128  , 127 ,128))\n",
        "        fake_img  = torch.nn.functional.pad(fake_img ,(127 , 128 ,127 ,128))\n",
        "\n",
        "\n",
        "        real_img = real_img.unsqueeze(dim = 0)\n",
        "        #fake_img = fake_img.unsqueeze(dim = 0)\n",
        "\n",
        "        fake_features.append(inception_model(fake_img))\n",
        "        real_features.append(inception_model(real_img))\n",
        "\n",
        "    real_matrix = torch.stack(real_features)\n",
        "    fake_matrix = torch.stack(fake_features)\n",
        "\n",
        "    real_mean = torch.mean(real_matrix , dim = 0)\n",
        "    fake_mean = torch.mean(fake_matrix , dim = 0)\n",
        "    real_matrix = real_matrix.squeeze(dim  = 1)\n",
        "    fake_matrix = fake_matrix.squeeze(dim  = 1)\n",
        "\n",
        "    real_cov = torch.cov( torch.transpose(real_matrix,0 , 1 ))\n",
        "    fake_cov = torch.cov( torch.transpose(fake_matrix, 0 , 1))\n",
        "\n",
        "    return (torch.square(torch.norm((real_mean - fake_mean), p ='fro')) + torch.trace(real_cov + fake_cov - 2*torch.sqrt(real_cov*fake_cov)))\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:12.43327Z",
          "iopub.execute_input": "2024-06-21T16:18:12.433925Z",
          "iopub.status.idle": "2024-06-21T16:18:13.644467Z",
          "shell.execute_reply.started": "2024-06-21T16:18:12.433895Z",
          "shell.execute_reply": "2024-06-21T16:18:13.643573Z"
        },
        "trusted": true,
        "id": "16AgIVitzzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self , num , dim):\n",
        "    super(Discriminator, self ).__init__()\n",
        "\n",
        "    # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "    # padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "    #Hout =⌊  ( Hin +2×padding[0]−dilation[0]×(kernel_size[0]−1)−1 / stride[0]) + 1  ⌋\n",
        "\n",
        "    #input is 64x64 and output is a sigmoid function\n",
        "\n",
        "    self.conv1 = spectral_norm((nn.Conv2d(3 , 64 ,kernel_size = 3 ,stride =  2 ,padding =  1 )))\n",
        "    self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "    self.elu1 = nn.LeakyReLU(0.1, inplace = False)\n",
        "    self.dropout1 = nn.Dropout2d(0.375)\n",
        "\n",
        "    self.conv2 = spectral_norm(nn.Conv2d(64 , 128,kernel_size=  3 ,stride =  2,padding =  1 ))\n",
        "    self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "    self.elu2 =  nn.LeakyReLU(0.1, inplace = False)\n",
        "    self.dropout2 = nn.Dropout2d(0.375)\n",
        "\n",
        "    self.conv3 = spectral_norm((nn.Conv2d(128, 256 ,kernel_size =  3 ,stride =  2 ,padding =  1 )))\n",
        "    self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "    self.elu3 = nn.LeakyReLU(0.1, inplace = False)\n",
        "    self.dropout3 = nn.Dropout2d(0.375)\n",
        "\n",
        "    self.conv4 = spectral_norm(nn.Conv2d(256 , 512 ,kernel_size =  3 ,stride =  2 , padding = 1 ))\n",
        "    self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "    self.elu4 =  nn.LeakyReLU(0.1, inplace = False)\n",
        "    self.dropout4 = nn.Dropout2d(0.375)\n",
        "\n",
        "\n",
        "    self.leakyrelu= (nn.LeakyReLU(negative_slope =0.2))\n",
        "    self.simple = spectral_norm(nn.Linear(8192, 1))\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.60)\n",
        "    self.weights_init()\n",
        "\n",
        "    # same layers as that of the generator\n",
        "  def weights_init(m):\n",
        "    print(\"init\")\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
        "        #self.weight_init(m.weight.data, mean=0.0, std=0.08)\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        #nn.init.normal_(m.weight, mean=0.0, std=0.02, generator=None)\n",
        "  def forward(self , img):\n",
        "    h1 =self.elu1((self.conv1(img)))\n",
        "    #h1 = self.dropout1(h1)\n",
        "    h2 =self.elu2((self.conv2(h1)))\n",
        "    #h2 = self.dropout2(h2)\n",
        "    h3 =self.elu3((self.conv3(h2)))\n",
        "    #h3 = self.dropout3(h3)\n",
        "    h4 =self.elu4((self.conv4(h3)))\n",
        "    #h4 = self.dropout4(h4)\n",
        "\n",
        "\n",
        "    h6 = torch.flatten(h4 , start_dim = 1)\n",
        "    #h6 = self.dropout(h6)\n",
        "    h7 = self.simple(h6)\n",
        "    return self.leakyrelu(h7) , h6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:13.645792Z",
          "iopub.execute_input": "2024-06-21T16:18:13.646131Z",
          "iopub.status.idle": "2024-06-21T16:18:13.66057Z",
          "shell.execute_reply.started": "2024-06-21T16:18:13.646093Z",
          "shell.execute_reply": "2024-06-21T16:18:13.659614Z"
        },
        "trusted": true,
        "id": "wuLm4QR1zzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "30snyvGszzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  # it takes in an input of a tensor (2-D) of size 8x8 in the paper it is a 1-D array , but for simplicity of convolution's dimensions i am taking y = x\n",
        "  #Hout =(Hin − 1 )×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n",
        "\n",
        "  #Wout =(Win − 1 )×stride[1]−2×padding[1]+dilation[1]×(kernel_size[1]−1)+output_padding[1]+1\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self , num , dim ):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "  # i will be inputting a tensor of size (batch_size , 1(channels) , num(y dimension) , num(x dimension))\n",
        "  # and i want a output of (batch_size , 3(channels) , dim(y dimension) , dim(x dimension))\n",
        "\n",
        "\n",
        "#torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "  #output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "    #take a noise of 100 size , then make it into 1024*4*4 , then reshape it into (1 , 1024 , 4 , 4)\n",
        "\n",
        "    # i am assuming that the covolutions with stride 1 are the reason behind the patternized outputs ... bcz i dont have anything else to blame\n",
        "    #that on and also the notebook which i saw was working had this only\n",
        "    self.linear = nn.Linear(256 , 16384)\n",
        "    self.elul =  nn.LeakyReLU(0.1, inplace = False)\n",
        "\n",
        "    self.convt1 = nn.ConvTranspose2d(in_channels =1024 , out_channels = 512 , kernel_size = 3 , stride = 2 , padding = 1, output_padding = 1)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(512)\n",
        "    self.elu1 =  nn.LeakyReLU(0.1, inplace = False)\n",
        "\n",
        "    self.convt2 = nn.ConvTranspose2d(512 , 256,kernel_size =  3 ,stride =  2  , padding = 1 , output_padding = 1 )\n",
        "    self.batchnorm2 = nn.BatchNorm2d(256)\n",
        "    self.elu2 =  nn.LeakyReLU(0.1, inplace = False)\n",
        "\n",
        "\n",
        "    self.convt3 = nn.ConvTranspose2d(256, 128 ,kernel_size =  3,stride =  2 , padding = 1 , output_padding = 1   )\n",
        "    self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "    self.elu3 = nn.LeakyReLU(0.1, inplace = False)\n",
        "\n",
        "\n",
        "    self.convt5 = nn.ConvTranspose2d(128 , 3 ,kernel_size = 3 ,stride =  2 ,padding =  1  , output_padding = 1)\n",
        "    self.batchnorm5 = nn.BatchNorm2d(1)\n",
        "    self.relu5 = nn.ReLU(inplace = False)\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    self.weights_init()\n",
        "  def weights_init(m):\n",
        "    print(\"initdone\")\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
        "        #self.weight_init(m.weight.data, mean=0.0, std=0.08)\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        #nn.init.normal_(m.weight, mean=0.0, std=0.02, generator=None)\n",
        "  def forward(self , z):\n",
        "    z = torch.flatten(z , start_dim = 1)\n",
        "    z1 = self.elul(self.linear(z))\n",
        "\n",
        "    z2 = z1.reshape(-1 , 1024 , 4, 4)\n",
        "    h1 =self.elu1(self.batchnorm1(self.convt1(z2)))\n",
        "    h2 =self.elu2((self.convt2(h1)))\n",
        "    h3 =self.elu3((self.convt3(h2)))\n",
        "\n",
        "    h5 =self.relu5((self.convt5(h3)))\n",
        "\n",
        "    h6 = self.tanh(h5)\n",
        "\n",
        "    return h6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:13.66278Z",
          "iopub.execute_input": "2024-06-21T16:18:13.663072Z",
          "iopub.status.idle": "2024-06-21T16:18:13.677357Z",
          "shell.execute_reply.started": "2024-06-21T16:18:13.663048Z",
          "shell.execute_reply": "2024-06-21T16:18:13.676436Z"
        },
        "trusted": true,
        "id": "3eSUbD7GzzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# first create a dataset , and a dataloader\n",
        "trans = transforms.Compose([\n",
        "\n",
        "    transforms.Resize((64,64)),\n",
        "    torchvision.transforms.ConvertImageDtype(torch.float32)\n",
        "\n",
        "    ])\n",
        "trans_for_mnist =  transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64,64)),\n",
        "    torchvision.transforms.ConvertImageDtype(torch.float32)\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "#dataset class\n",
        "subset_size = 440\n",
        "class img_dataset(Dataset):\n",
        "  def __init__(self , img_dir , trans , subset_size):\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = trans\n",
        "    self.files = os.listdir(self.img_dir)\n",
        "    self.data = []\n",
        "    self.subset_size = subset_size\n",
        "    for idx , file in enumerate(self.files):\n",
        "        if (idx > subset_size):\n",
        "            break\n",
        "        img = read_image(os.path.join(self.img_dir , file))\n",
        "        img = trans(img)\n",
        "        self.data.append(img)\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.subset_size\n",
        "  def __getitem__(self ,idx ):\n",
        "    img = self.data[idx]\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:13.678454Z",
          "iopub.execute_input": "2024-06-21T16:18:13.678787Z",
          "iopub.status.idle": "2024-06-21T16:18:13.688872Z",
          "shell.execute_reply.started": "2024-06-21T16:18:13.678756Z",
          "shell.execute_reply": "2024-06-21T16:18:13.687994Z"
        },
        "trusted": true,
        "id": "afSmFOG5zzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_func = nn.BCELoss()\n",
        "generator = Generator(4 , 64)\n",
        "discriminator = Discriminator (4, 64)\n",
        "\n",
        "#trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=trans_for_mnist)\n",
        "dataset = img_dataset(\"/kaggle/input/gan-large-area-png\", trans , subset_size)\n",
        "#dataset = trainset\n",
        "batch = 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#device thing\n",
        "# Assuming your model is named 'model'\n",
        "\n",
        "generator = generator.to(device)\n",
        "#discriminator.spectral_norm()\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "#dataset = dataset.to(device)\n",
        "\n",
        "\n",
        "g_optim = torch.optim.Adam(generator.parameters() , lr = 0.00002 , betas = (0 , 0.9) )\n",
        "d_optim = torch.optim.Adam(discriminator.parameters() , lr = 0.0000375, betas = (0 , 0.9) )\n",
        "\n",
        "#d_optim = torch.optim.(discriminator.parameters() , lr = 0.00005, betas = (0.5 , 0.999))\n",
        "#g_optim = torch.optim.Adam(generator.parameters() , lr = 0.00005, betas = (0.5 , 0.999))\n",
        "\n",
        "g_scheduler = lr_scheduler.ExponentialLR(g_optim, gamma=0.998)\n",
        "d_scheduler = lr_scheduler.ExponentialLR(d_optim, gamma=0.998)\n",
        "\n",
        "subset  = Subset(dataset , list(range(subset_size)))\n",
        "data_loader = DataLoader(subset, batch_size = batch , shuffle = True , pin_memory = True)\n",
        "\n",
        "\n",
        "#shapes\n",
        "# z = (batch, 1 , 8 , 8)\n",
        "z = torch.normal(mean = 0 , std = 1 , size = (batch ,1, 16 , 16)).to(device)\n",
        "print(\"shape of generator output = \" + str((generator(z).shape)))\n",
        "print(\"shape of discriminator output = \" + str(discriminator(generator(z))[0].shape))\n",
        "\n",
        "\n",
        "print(dataset.__len__())\n",
        "\n",
        "for i in range(5):\n",
        "    img = dataset[i]\n",
        "    plt.imshow(img.permute(1,2,0))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:13.689883Z",
          "iopub.execute_input": "2024-06-21T16:18:13.690191Z",
          "iopub.status.idle": "2024-06-21T16:18:17.339993Z",
          "shell.execute_reply.started": "2024-06-21T16:18:13.690166Z",
          "shell.execute_reply": "2024-06-21T16:18:17.339138Z"
        },
        "trusted": true,
        "id": "pGW9jYNZzzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.__len__())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:17.341007Z",
          "iopub.execute_input": "2024-06-21T16:18:17.341313Z",
          "iopub.status.idle": "2024-06-21T16:18:17.346502Z",
          "shell.execute_reply.started": "2024-06-21T16:18:17.341286Z",
          "shell.execute_reply": "2024-06-21T16:18:17.345616Z"
        },
        "trusted": true,
        "id": "lYmV0-8izzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(generator , input_size = (1,16,16))\n",
        "summary(discriminator , input_size = (3,64,64))\n",
        "summary(inception_model, input_size = (3, 299 , 299))\n",
        "#remember the discriminator parameters shown here are double of the actual number bcz of the spectral norm thing\n",
        "generator = nn.DataParallel(generator)\n",
        "discriminator = nn.DataParallel(discriminator)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:17.347728Z",
          "iopub.execute_input": "2024-06-21T16:18:17.348014Z",
          "iopub.status.idle": "2024-06-21T16:18:17.64915Z",
          "shell.execute_reply.started": "2024-06-21T16:18:17.347991Z",
          "shell.execute_reply": "2024-06-21T16:18:17.648148Z"
        },
        "trusted": true,
        "id": "loK5G4uDzzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3000\n",
        "iter = 4\n",
        "\n",
        "ones = torch.ones(batch,1).to(device)\n",
        "zeroes = torch.zeros(batch,1).to(device)\n",
        "fidlist = []\n",
        "generator_loss = []\n",
        "discriminator_loss = []\n",
        "epoch_count = []\n",
        "discriminator_loss_part1 = []\n",
        "discriminator_loss_part2 = []\n",
        "feature_loss_list = []\n",
        "clip_value = 1\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:17.650526Z",
          "iopub.execute_input": "2024-06-21T16:18:17.650939Z",
          "iopub.status.idle": "2024-06-21T16:18:17.657501Z",
          "shell.execute_reply.started": "2024-06-21T16:18:17.650904Z",
          "shell.execute_reply": "2024-06-21T16:18:17.656583Z"
        },
        "trusted": true,
        "id": "rCEprJ5KzzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "print(ones.dtype)\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "feature_loss =  torch.nn.MSELoss()\n",
        "factor = 0\n",
        "for epoch in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "\n",
        "  factor = factor*0.995\n",
        "  #print(epoch)\n",
        "  epoch_count.append(epoch)\n",
        "  generator_loss.append(0)\n",
        "  discriminator_loss.append(0)\n",
        "  discriminator_loss_part1.append(0)\n",
        "  discriminator_loss_part2.append(0)\n",
        "  feature_loss_list.append(0)\n",
        "  fidlist.append(fid(50 , dataset , inception_model , generator).item())\n",
        "\n",
        "  for batch_idx , (data) in enumerate(data_loader):\n",
        "    if(batch_idx % iter == 0):\n",
        "        generator.train()\n",
        "        discriminator.eval()\n",
        "\n",
        "        #print(\"gen----------------------------------------------\" , batch_idx)\n",
        "        batch_size = (data.shape[0])\n",
        "        data = data.to(device)\n",
        "        z = torch.normal(mean = 0 , std = 1 ,size = (batch_size , 1 , 16,16) ).to(device)\n",
        "        discriminator_output_generated , discriminator_features_generated = discriminator(generator(z))\n",
        "        discriminator_output_real, discriminator_features_real = discriminator(data)\n",
        "\n",
        "        f_loss = feature_loss(discriminator_features_generated , discriminator_features_real)\n",
        "        feature_loss_list[epoch] += f_loss.item()\n",
        "        g_loss = torch.mean(-1*(discriminator_output_generated))\n",
        "\n",
        "        g_loss = (1-factor)*g_loss+ factor*f_loss\n",
        "        generator_loss[epoch] += torch.mean(g_loss).item()\n",
        "        d_optim.zero_grad()\n",
        "\n",
        "        g_optim.zero_grad()\n",
        "\n",
        "        g_loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(generator.parameters(), clip_value)\n",
        "        g_optim.step()\n",
        "    else:\n",
        "        generator.eval()\n",
        "        discriminator.train()\n",
        "        #print(\"dis\" , batch_idx)\n",
        "        #print(batch_idx)\n",
        "        batch_size = (data.shape[0])\n",
        "        data = data.to(device)\n",
        "\n",
        "\n",
        "        z = torch.normal(mean = 0 , std = 1 ,size = (batch_size , 1 , 16,16)).to(device)\n",
        "        discriminator_output_real = discriminator(data)[0]\n",
        "        discriminator_output_generated = discriminator(generator(z))[0]\n",
        "        part1 = (discriminator_output_real)\n",
        "        part2 = ( discriminator_output_generated)\n",
        "        #d_loss = loss_func(discriminator_output_real, ones) + loss_func( discriminator_output_generated, zeroes)\n",
        "        d_loss = torch.mean(-1*(part1 - part2))\n",
        "        discriminator_loss[epoch] += (d_loss).item()/(iter -1)\n",
        "        discriminator_loss_part1[epoch] += -1*torch.mean(part1).item()/(iter-1)\n",
        "\n",
        "        discriminator_loss_part2[epoch] += torch.mean(part2).item()/(iter-1)\n",
        "        d_optim.zero_grad()\n",
        "        d_loss.backward()\n",
        "\n",
        "        d_optim.step()\n",
        "\n",
        "   #torch.nn.utils.clip_grad_norm_(generator.parameters(), clip_value)\n",
        "\n",
        "    if(epoch%2 == 0):\n",
        "\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                with torch.no_grad():\n",
        "                    generator.eval()\n",
        "                    sample_z = torch.normal(mean=0, std=1, size=(1, 1, 16, 16)).to(device)\n",
        "                    generated_img = generator(sample_z).cpu()\n",
        "\n",
        "                    # Convert the image tensor to [0, 1] range and save it\n",
        "                    img_path = f'images/epoch_{epoch}_batch_{batch_idx}.png'\n",
        "                    vutils.save_image(generated_img, img_path, normalize=True)\n",
        "\n",
        "\n",
        "  #g_scheduler.step()\n",
        "  #d_scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    #print(str(data.min()) + \"   \" + str(data.max()))\n",
        "    #print(str(generator(z).min()) + \"  \" + str(generator(z).max()))\n",
        "\n",
        "  print(f\"epoch - : {epoch_count[epoch]},Generator loss - : {generator_loss[epoch]}, Discriminator - : {discriminator_loss[epoch]} , g_lr = {g_optim.param_groups[0]['lr']} , d_lr = {d_optim.param_groups[0]['lr']}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T16:18:40.888482Z",
          "iopub.execute_input": "2024-06-21T16:18:40.888834Z",
          "iopub.status.idle": "2024-06-21T17:56:06.877383Z",
          "shell.execute_reply.started": "2024-06-21T16:18:40.888808Z",
          "shell.execute_reply": "2024-06-21T17:56:06.876479Z"
        },
        "trusted": true,
        "id": "sy1VOvTBzzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8hb6zwVPzzD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for saving images\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming 'discriminator' is your pre-trained model\n",
        "\n",
        "# Define the device for computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load pre-trained model\n",
        "model = discriminator\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function (for example, maximizing activation of a certain layer)\n",
        "loss_activation = []\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Initialize input image (random noise or blank image)\n",
        "input_img = torch.randn(1, 3, 64, 64, device=device, requires_grad=True)  # or initialize with zeros or random noise\n",
        "input_img = input_img.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam([input_img], lr=0.0001)\n",
        "\n",
        "# Optimization loop\n",
        "num_iterations = 100  # Increase iterations for more refined result\n",
        "for i in tqdm(range(num_iterations)):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_img)[0]\n",
        "\n",
        "    # Maximize the activation of the chosen layer (example)\n",
        "    loss = -torch.mean(model(input_img)[0])\n",
        "    loss_activation.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 10 == 0:  # Adjust as needed to save images periodically\n",
        "        # Convert optimized input image to PIL image for visualization\n",
        "        output_img = transforms.ToPILImage()(input_img.squeeze().detach().cpu())\n",
        "\n",
        "        # Save the image using matplotlib\n",
        "        plt.imshow(output_img)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(f'output_{i+1}.png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "# Plot loss vs iterations2\n",
        "plt.figure()\n",
        "plt.plot(range(num_iterations), loss_activation, label='Loss Activation')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss Activation')\n",
        "plt.title('Loss Activation vs Iterations')\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_vs_iterations.png', bbox_inches='tight')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:25.243857Z",
          "iopub.execute_input": "2024-06-21T18:01:25.244293Z",
          "iopub.status.idle": "2024-06-21T18:01:27.765228Z",
          "shell.execute_reply.started": "2024-06-21T18:01:25.244261Z",
          "shell.execute_reply": "2024-06-21T18:01:27.764422Z"
        },
        "trusted": true,
        "id": "r9cpVaPlzzD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data for demonstration\n",
        "x_values = epoch_count\n",
        "y1_values = generator_loss\n",
        "y2_values = discriminator_loss\n",
        "y3_values = discriminator_loss_part1\n",
        "y4_values = discriminator_loss_part2\n",
        "y5_values = feature_loss_list\n",
        "\n",
        "# Plotting the first line\n",
        "plt.plot(x_values, y1_values, label='generator_loss')\n",
        "\n",
        "# Plotting the second line\n",
        "plt.plot(x_values, y2_values, label='discriminator_loss')\n",
        "\n",
        "plt.plot(x_values, y3_values, label='discriminator_loss_part1')\n",
        "\n",
        "\n",
        "# Plotting the second line\n",
        "plt.plot(x_values, y4_values, label='discriminator_loss_part2')\n",
        "\n",
        "plt.plot(x_values , y5_values , label= 'feature_loss')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses')\n",
        "plt.title('Loss Graphs')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "clipped_discriminator = [40 if x > 40 else -40 if x < -40 else x for x in discriminator_loss]\n",
        "plt.plot(epoch_count , clipped_discriminator , label = 'clipped_discriminator')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Discriminator_loss')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(x_values, y1_values, label='generator_loss')\n",
        "\n",
        "# Plotting the second line\n",
        "plt.plot(x_values, y2_values, label='discriminator_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss graph')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Sample data for demonstration\n",
        "x_values = epoch_count\n",
        "y1_values = discriminator_loss_part1\n",
        "y2_values = discriminator_loss_part2\n",
        "# Plotting the first line\n",
        "plt.plot(x_values, y1_values, label='discriminator_loss_part1')\n",
        "\n",
        "\n",
        "# Plotting the second line\n",
        "plt.plot(x_values, y2_values, label='discriminator_loss_part2')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('discriminator_losses_broken_down')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:32.694036Z",
          "iopub.execute_input": "2024-06-21T18:01:32.694862Z",
          "iopub.status.idle": "2024-06-21T18:01:34.143893Z",
          "shell.execute_reply.started": "2024-06-21T18:01:32.69483Z",
          "shell.execute_reply": "2024-06-21T18:01:34.142959Z"
        },
        "trusted": true,
        "id": "dgdzh1SGzzD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1_values = fidlist\n",
        "plt.plot(x_values, y1_values, label='fid score')\n",
        "\n",
        "# Plotting the second line\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.title('FID score ')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "y1_values = [min(x , 100) for x in fidlist]\n",
        "plt.plot(x_values, y1_values, label='cropped fid score')\n",
        "\n",
        "# Plotting the second line\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.title('Cropped FID score ')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:42.357503Z",
          "iopub.execute_input": "2024-06-21T18:01:42.3579Z",
          "iopub.status.idle": "2024-06-21T18:01:42.968016Z",
          "shell.execute_reply.started": "2024-06-21T18:01:42.357868Z",
          "shell.execute_reply": "2024-06-21T18:01:42.967049Z"
        },
        "trusted": true,
        "id": "DBg0v-FkzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1_values = feature_loss_list\n",
        "plt.plot(x_values, y1_values, label='feature_losses')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Feature Loss graph')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "#cropped graph\n",
        "\n",
        "y1_values= [min(x , 50) for x in feature_loss_list]\n",
        "plt.plot(x_values, y1_values, label='cropped feature_losses')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Cropped Feature Loss graph')\n",
        "\n",
        "# Adding a legend to differentiate lines\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:47.056655Z",
          "iopub.execute_input": "2024-06-21T18:01:47.056997Z",
          "iopub.status.idle": "2024-06-21T18:01:47.532407Z",
          "shell.execute_reply.started": "2024-06-21T18:01:47.056971Z",
          "shell.execute_reply": "2024-06-21T18:01:47.53165Z"
        },
        "trusted": true,
        "id": "0uass1iszzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2Jr3qHUzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save a list to a file\n",
        "\n",
        "with open('generator_loss.pkl', 'wb') as f:\n",
        "    pickle.dump(generator_loss, f)\n",
        "\n",
        "\n",
        "with open('discriminator_loss.pkl', 'wb') as f:\n",
        "    pickle.dump(discriminator_loss, f)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:51.884634Z",
          "iopub.execute_input": "2024-06-21T18:01:51.885472Z",
          "iopub.status.idle": "2024-06-21T18:01:51.89141Z",
          "shell.execute_reply.started": "2024-06-21T18:01:51.885439Z",
          "shell.execute_reply": "2024-06-21T18:01:51.89048Z"
        },
        "trusted": true,
        "id": "YGUQfWqPzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "base = generator(z)[0]\n",
        "lista = []\n",
        "for i in range(1000):\n",
        "        z = torch.normal(mean = 0 , std = 1, size = (32 , 1 , 16 , 16)).to(device)\n",
        "        img = generator(z)[0]\n",
        "        diff = torch.sum(base - img)\n",
        "        lista.append(diff.item())\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:01:56.900817Z",
          "iopub.execute_input": "2024-06-21T18:01:56.901204Z",
          "iopub.status.idle": "2024-06-21T18:02:05.380435Z",
          "shell.execute_reply.started": "2024-06-21T18:01:56.901173Z",
          "shell.execute_reply": "2024-06-21T18:02:05.379445Z"
        },
        "trusted": true,
        "id": "OkpxyRtozzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lista))\n",
        "plt.scatter(lista, range(0, len(lista) ))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:16.541204Z",
          "iopub.execute_input": "2024-06-21T18:02:16.54158Z",
          "iopub.status.idle": "2024-06-21T18:02:16.827488Z",
          "shell.execute_reply.started": "2024-06-21T18:02:16.541551Z",
          "shell.execute_reply": "2024-06-21T18:02:16.826509Z"
        },
        "trusted": true,
        "id": "azVhNonwzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "\n",
        "        z = torch.normal(mean = 0 , std = 1 , size = (batch ,1, 16 , 16)).to(device)\n",
        "        img = generator(z)[0].to('cpu').detach()\n",
        "        img  = img.permute(1,2,0)\n",
        "        print(torch.min(img) , \"    \" , torch.max(img))\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:20.255052Z",
          "iopub.execute_input": "2024-06-21T18:02:20.255443Z",
          "iopub.status.idle": "2024-06-21T18:02:22.291799Z",
          "shell.execute_reply.started": "2024-06-21T18:02:20.255411Z",
          "shell.execute_reply": "2024-06-21T18:02:22.290908Z"
        },
        "trusted": true,
        "id": "gcOc6QqkzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fidlist"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:27.505975Z",
          "iopub.execute_input": "2024-06-21T18:02:27.506855Z",
          "iopub.status.idle": "2024-06-21T18:02:27.525015Z",
          "shell.execute_reply.started": "2024-06-21T18:02:27.506821Z",
          "shell.execute_reply": "2024-06-21T18:02:27.524072Z"
        },
        "trusted": true,
        "id": "uEBuL0YyzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fidlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:38.010233Z",
          "iopub.execute_input": "2024-06-21T18:02:38.010604Z",
          "iopub.status.idle": "2024-06-21T18:02:38.016674Z",
          "shell.execute_reply.started": "2024-06-21T18:02:38.010575Z",
          "shell.execute_reply": "2024-06-21T18:02:38.015692Z"
        },
        "trusted": true,
        "id": "EcJeZFi4zzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fidlist[2995:2999]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:35.684519Z",
          "iopub.execute_input": "2024-06-21T18:02:35.685488Z",
          "iopub.status.idle": "2024-06-21T18:02:35.691507Z",
          "shell.execute_reply.started": "2024-06-21T18:02:35.685435Z",
          "shell.execute_reply": "2024-06-21T18:02:35.690509Z"
        },
        "trusted": true,
        "id": "HvUFs3XBzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(fidlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:02:40.025935Z",
          "iopub.execute_input": "2024-06-21T18:02:40.026662Z",
          "iopub.status.idle": "2024-06-21T18:02:40.032818Z",
          "shell.execute_reply.started": "2024-06-21T18:02:40.026621Z",
          "shell.execute_reply": "2024-06-21T18:02:40.031907Z"
        },
        "trusted": true,
        "id": "Bb2ocUlAzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r file.zip /kaggle/working\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:07:04.172027Z",
          "iopub.execute_input": "2024-06-21T18:07:04.172947Z",
          "iopub.status.idle": "2024-06-21T18:07:05.515564Z",
          "shell.execute_reply.started": "2024-06-21T18:07:04.172912Z",
          "shell.execute_reply": "2024-06-21T18:07:05.514548Z"
        },
        "trusted": true,
        "id": "I6GJvg7tzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:07:29.47295Z",
          "iopub.execute_input": "2024-06-21T18:07:29.473344Z",
          "iopub.status.idle": "2024-06-21T18:07:30.450503Z",
          "shell.execute_reply.started": "2024-06-21T18:07:29.473314Z",
          "shell.execute_reply": "2024-06-21T18:07:30.449559Z"
        },
        "trusted": true,
        "id": "EuXVyKyPzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'file.zip')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-21T18:07:44.769358Z",
          "iopub.execute_input": "2024-06-21T18:07:44.770267Z",
          "iopub.status.idle": "2024-06-21T18:07:44.778203Z",
          "shell.execute_reply.started": "2024-06-21T18:07:44.770222Z",
          "shell.execute_reply": "2024-06-21T18:07:44.777228Z"
        },
        "trusted": true,
        "id": "10yqvqDfzzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d9EY5kNzzEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}